diff --git a/.idea/graduation3.iml b/.idea/graduation3.iml
index 707eda7..9aa7215 100644
--- a/.idea/graduation3.iml
+++ b/.idea/graduation3.iml
@@ -2,7 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="jdk" jdkName="Python 3.6 (new)" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Python 3.6 (DQEAF)" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
   <component name="TestRunnerService">
diff --git a/.idea/misc.xml b/.idea/misc.xml
index 9dacc81..18bd36b 100644
--- a/.idea/misc.xml
+++ b/.idea/misc.xml
@@ -3,5 +3,5 @@
   <component name="PreferredVcsStorage">
     <preferredVcsName>ApexVCS</preferredVcsName>
   </component>
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.6 (new)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.6 (DQEAF)" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
diff --git a/hook/plot_hook.py b/hook/plot_hook.py
index 04eb1ed..9f00d72 100644
--- a/hook/plot_hook.py
+++ b/hook/plot_hook.py
@@ -54,14 +54,25 @@ class PlotHook(StepHook):
         else:
             self.vis.line(Y=Y, X=X, win=self.win, update='append', opts=self.opts)
 
-    def __call__(self, env, agent, step):
-        if self.plot_index == 2:
-            self.episode_step += 1
-            if env.current_reward == 10:
-                d = {'Average Reward': 10 / self.episode_step}
+    def __call__(self, env, agent, step, value=0):
+        if self.plot_index == 0 or self.plot_index == 1:
+            if step % 10 == 0:
+                stat = agent.get_statistics()
+                d = {stat[self.plot_index][0]: stat[self.plot_index][1]}
                 self.plot(step, d)
-                self.episode_step = 0
-
+        elif self.plot_index == 2 or self.plot_index == 3:
+            stat = agent.get_statistics()
+            d = {stat[self.plot_index - 2][0]: stat[self.plot_index - 2][1]}
+            self.plot(step, d)
+        elif self.plot_index == 4:
+            d = {'Steps to finish (train)': value}
+            self.plot(step, d)
+        elif self.plot_index == 5:
+            d = {'Steps to finish (test)': value}
+            self.plot(step, d)
+        elif self.plot_index == 6:
+            d = {'success rate': value}
+            self.plot(step, d)
         else:
             if step % 10 == 0:
                 stat = agent.get_statistics()
diff --git a/my_chainer/my_evaluator.py b/my_chainer/my_evaluator.py
index b969490..dd68e7b 100644
--- a/my_chainer/my_evaluator.py
+++ b/my_chainer/my_evaluator.py
@@ -4,6 +4,7 @@ from __future__ import division
 from __future__ import absolute_import
 from builtins import *  # NOQA
 from future import standard_library
+
 standard_library.install_aliases()
 
 import logging
@@ -14,7 +15,6 @@ import time
 
 import numpy as np
 
-
 """Columns that describe information about an experiment.
 
 steps: number of time steps taken (= number of actions taken)
@@ -31,7 +31,7 @@ _basic_columns = ('steps', 'episodes', 'elapsed', 'mean',
 
 
 def run_evaluation_episodes(env, agent, n_runs, max_episode_len=None,
-                            explorer=None, logger=None):
+                            explorer=None, logger=None, test_hooks=[]):
     """Run multiple evaluation episodes and return returns.
 
     Args:
@@ -58,6 +58,7 @@ def run_evaluation_episodes(env, agent, n_runs, max_episode_len=None,
         while not (done or t == max_episode_len):
             def greedy_action_func():
                 return agent.act(obs)
+
             if explorer is not None:
                 a = explorer.select_action(t, greedy_action_func)
             else:
@@ -65,16 +66,19 @@ def run_evaluation_episodes(env, agent, n_runs, max_episode_len=None,
             obs, r, done, info = env.step(a)
             test_r += r
             t += 1
+        test_hooks[1](env, agent, i, t)
         agent.stop_episode()
         # As mixing float and numpy float causes errors in statistics
         # functions, here every score is cast to float.
+        if (test_r > 0):
+            test_r = 10
         scores.append(float(test_r))
         logger.info('test episode: %s R: %s', i, test_r)
     return scores
 
 
 def eval_performance(env, agent, n_runs, max_episode_len=None,
-                     explorer=None, logger=None):
+                     explorer=None, logger=None, test_hooks=[]):
     """Run multiple evaluation episodes and return statistics.
 
     Args:
@@ -95,7 +99,7 @@ def eval_performance(env, agent, n_runs, max_episode_len=None,
         env, agent, n_runs,
         max_episode_len=max_episode_len,
         explorer=explorer,
-        logger=logger)
+        logger=logger, test_hooks=test_hooks)
     stats = dict(
         mean=statistics.mean(scores),
         median=statistics.median(scores),
@@ -127,7 +131,7 @@ class Evaluator(object):
 
     def __init__(self, agent, env, n_runs, eval_interval,
                  outdir, max_episode_len=None, explorer=None,
-                 step_offset=0, logger=None):
+                 step_offset=0, test_hooks=[], logger=None):
         self.agent = agent
         self.env = env
         self.max_score = np.finfo(np.float32).min
@@ -152,7 +156,7 @@ class Evaluator(object):
         eval_stats = eval_performance(
             self.env, self.agent, self.n_runs,
             max_episode_len=self.max_episode_len, explorer=self.explorer,
-            logger=self.logger)
+            logger=self.logger, test_hooks=self.test_hooks)
         elapsed = time.time() - self.start_time
         custom_values = tuple(tup[1] for tup in self.agent.get_statistics())
         mean = eval_stats['mean']
diff --git a/train.py b/train.py
index 2001791..f53fcac 100644
--- a/train.py
+++ b/train.py
@@ -20,6 +20,7 @@ from gym_malware.envs.controls import manipulate2 as manipulate
 from gym_malware.envs.utils import pefeatures
 from hook.plot_hook import PlotHook
 from hook.training_scores_hook import TrainingScoresHook
+from my_chainer import train_agent
 
 ACTION_LOOKUP = {i: act for i, act in enumerate(manipulate.ACTION_TABLE.keys())}
 
@@ -169,25 +170,35 @@ def main():
 
         agent = create_ddqn_agent(env, args)
 
-        q_hook = PlotHook('Average Q Value', ylabel='Average Action Value (Q)')
-        loss_hook = PlotHook('Average Loss', plot_index=1, ylabel='Average Loss per Episode')
-        reward_hook = PlotHook('Average Reward', plot_index=2, ylabel='Reward Value per Episode')
-        scores_hook = TrainingScoresHook('scores.txt', args.outdir)
-
-        chainerrl.experiments.train_agent_with_evaluation(
+        step_q_hook = PlotHook('Average Q Value (Step)', plot_index=0, xlabel='train step',
+                               ylabel='Average Q Value (Step)')
+        step_loss_hook = PlotHook('Average Loss (Step)', plot_index=1, xlabel='train step',
+                                  ylabel='Average Loss (Step)')
+        episode_q_hook = PlotHook('Average Q Value (Episode)', plot_index=2, xlabel='train episode',
+                                  ylabel='Average Q Value (Episode)')
+        episode_loss_hook = PlotHook('Average Loss (Episode)', plot_index=3, xlabel='train episode',
+                                     ylabel='Average Loss (Episode)')
+        episode_finish_hook = PlotHook('Steps to finish (train)', plot_index=4, xlabel='train episode',
+                                       ylabel='Steps to finish (train)')
+        test_finish_hook = PlotHook('Steps to finish (test)', plot_index=5, xlabel='test episode',
+                                    ylabel='Steps to finish (test)')
+        test_scores_hook = PlotHook('success rate', plot_index=6, xlabel='test epoch', ylabel='success rate')
+
+        train_agent().train_agent_with_evaluation(
             agent, env,
             steps=args.steps,  # Train the graduation_agent for this many rounds steps
             max_episode_len=env.maxturns,  # Maximum length of each episodes
             eval_interval=args.eval_interval,  # Evaluate the graduation_agent after every 1000 steps
             eval_n_runs=args.eval_n_runs,  # 100 episodes are sampled for each evaluation
             outdir=args.outdir,  # Save everything to 'result' directory
-            step_hooks=[q_hook, loss_hook, scores_hook, reward_hook],
+            episode_hooks=[episode_q_hook, episode_loss_hook, episode_finish_hook],
+            test_hooks=[test_scores_hook, test_finish_hook],
             successful_score=7,
             eval_env=test_env
         )
 
         # 保证训练一轮就成功的情况下能成功打印scores.txt文件
-        scores_hook(None, None, 1000)
+        # scores_hook(None, None, 1000)
 
         return env, agent
 
@@ -216,6 +227,7 @@ def main():
             print('Output files are saved in {}'.format(args.outdir))
 
             env, agent = train_agent(args)
+            print(env.history);
 
             with open(os.path.join(args.outdir, 'scores.txt'), 'a') as f:
                 f.write(
